{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL3-ms2NrjXc",
        "outputId": "7cf7170e-67b8-404f-8028-de5b766ed15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GJAMQwV6CPj",
        "outputId": "26518075-1129-482c-e85f-7490c2c1e5f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tsfresh in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (2.2.2)\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (0.14.4)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.5.2)\n",
            "Requirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (4.66.6)\n",
            "Requirement already satisfied: stumpy>=1.7.2 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.13.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from tsfresh) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from tsfresh) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.0->tsfresh) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.0->tsfresh) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.0->tsfresh) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.9.1->tsfresh) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->tsfresh) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.0->tsfresh) (3.5.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13->tsfresh) (24.2)\n",
            "Requirement already satisfied: numba>=0.57.1 in /usr/local/lib/python3.10/dist-packages (from stumpy>=1.7.2->tsfresh) (0.60.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57.1->stumpy>=1.7.2->tsfresh) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.0->tsfresh) (1.16.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm) (1.14.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tsfresh\n",
        "!pip install lightgbm\n",
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7shtxy04thc"
      },
      "outputs": [],
      "source": [
        "# Required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import joblib\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Advanced data processing\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    TimeSeriesSplit,\n",
        "    RandomizedSearchCV,\n",
        "    GridSearchCV\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Feature engineering\n",
        "from sklearn.decomposition import PCA\n",
        "import traceback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import r2_score\n",
        "import traceback\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8d3Y0hA51a3"
      },
      "outputs": [],
      "source": [
        "class SolarPowerAnalysis:\n",
        "    def __init__(self):\n",
        "        self.generation_data = None\n",
        "        self.weather_data = None\n",
        "        self.merged_data = None\n",
        "        self.data_reduction_log = {}\n",
        "\n",
        "    def log_data_reduction(self, step_name, data, previous_shape=None):\n",
        "        \"\"\"\n",
        "        Log data reduction steps with detailed information\n",
        "        \"\"\"\n",
        "        current_shape = data.shape[0]\n",
        "\n",
        "        # Handle the reduction calculation\n",
        "        if previous_shape is None or previous_shape == 0:\n",
        "            reduction = 0\n",
        "            reduction_pct = 0\n",
        "        else:\n",
        "            reduction = previous_shape - current_shape\n",
        "            reduction_pct = (reduction / previous_shape) * 100\n",
        "\n",
        "        self.data_reduction_log[step_name] = {\n",
        "            'previous_rows': previous_shape,\n",
        "            'current_rows': current_shape,\n",
        "            'rows_reduced': reduction,\n",
        "            'reduction_percentage': reduction_pct\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{step_name}:\")\n",
        "        print(f\"Previous rows: {previous_shape if previous_shape else 'N/A'}\")\n",
        "        print(f\"Current rows: {current_shape}\")\n",
        "        if previous_shape and previous_shape > 0:\n",
        "            print(f\"Reduction: {reduction} rows ({reduction_pct:.2f}%)\")\n",
        "\n",
        "    def load_data(self, generation_path, weather_path):\n",
        "        \"\"\"\n",
        "        Load and perform initial data processing\n",
        "        \"\"\"\n",
        "        # Load data\n",
        "        self.generation_data = pd.read_csv(generation_path)\n",
        "        self.weather_data = pd.read_csv(weather_path)\n",
        "\n",
        "        # Log initial data sizes\n",
        "        self.log_data_reduction(\"Initial Generation Data Load\", self.generation_data)\n",
        "        self.log_data_reduction(\"Initial Weather Data Load\", self.weather_data)\n",
        "\n",
        "        # Convert timestamps\n",
        "        for df in [self.generation_data, self.weather_data]:\n",
        "            if not pd.api.types.is_datetime64_any_dtype(df['DATE_TIME']):\n",
        "                df['DATE_TIME'] = pd.to_datetime(df['DATE_TIME'])\n",
        "\n",
        "        # Print sampling frequencies\n",
        "        gen_freq = self.generation_data['DATE_TIME'].diff().median()\n",
        "        weather_freq = self.weather_data['DATE_TIME'].diff().median()\n",
        "        print(f\"\\nGeneration data frequency: {gen_freq}\")\n",
        "        print(f\"Weather data frequency: {weather_freq}\")\n",
        "\n",
        "    def _resample_and_align_data(self):\n",
        "        \"\"\"\n",
        "        Enhanced resampling and alignment with data preservation\n",
        "        \"\"\"\n",
        "        print(\"\\nStarting data resampling and alignment...\")\n",
        "\n",
        "        # 1. Create a complete timestamp range with the higher frequency\n",
        "        time_range = pd.date_range(\n",
        "            start=min(self.generation_data['DATE_TIME'].min(),\n",
        "                      self.weather_data['DATE_TIME'].min()),\n",
        "            end=max(self.generation_data['DATE_TIME'].max(),\n",
        "                    self.weather_data['DATE_TIME'].max()),\n",
        "            freq='15min'\n",
        "        )\n",
        "\n",
        "        # 2. Process generation data\n",
        "        gen_before = self.generation_data.shape[0]\n",
        "\n",
        "        # First level aggregation with simplified aggregation\n",
        "        gen_grouped = self.generation_data.groupby(['DATE_TIME', 'SOURCE_KEY']).agg({\n",
        "            'DC_POWER': ['mean', 'std', 'min', 'max'],\n",
        "            'AC_POWER': ['mean', 'std', 'min', 'max'],\n",
        "            'DAILY_YIELD': 'mean',  # Simplified aggregation\n",
        "            'TOTAL_YIELD': 'mean'   # Simplified aggregation\n",
        "        }).reset_index()\n",
        "\n",
        "        # Flatten first level columns for easier access\n",
        "        gen_grouped.columns = ['DATE_TIME', 'SOURCE_KEY'] + [\n",
        "            f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) else col\n",
        "            for col in gen_grouped.columns[2:]\n",
        "        ]\n",
        "\n",
        "        # Plant-level aggregation with simpler aggregation functions\n",
        "        agg_functions = {\n",
        "            'DC_POWER_mean': ['sum', 'mean', 'std', 'min', 'max'],\n",
        "            'AC_POWER_mean': ['sum', 'mean', 'std', 'min', 'max'],\n",
        "            'DC_POWER_std': 'mean',\n",
        "            'DC_POWER_min': 'min',\n",
        "            'DC_POWER_max': 'max',\n",
        "            'AC_POWER_std': 'mean',\n",
        "            'AC_POWER_min': 'min',\n",
        "            'AC_POWER_max': 'max',\n",
        "            'DAILY_YIELD_mean': 'mean',  # Simplified aggregation\n",
        "            'TOTAL_YIELD_mean': 'mean'   # Simplified aggregation\n",
        "        }\n",
        "\n",
        "        # Aggregate data at the plant level\n",
        "        gen_agg = gen_grouped.groupby('DATE_TIME').agg(agg_functions).reset_index()\n",
        "\n",
        "        # Flatten column names after aggregation\n",
        "        gen_agg.columns = ['DATE_TIME'] + [\n",
        "            f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) else col\n",
        "            for col in gen_agg.columns[1:]\n",
        "        ]\n",
        "\n",
        "        # Column name mapping for consistency\n",
        "        column_mapping = {\n",
        "            'DC_POWER_mean_sum': 'DC_POWER_sum',\n",
        "            'AC_POWER_mean_sum': 'AC_POWER_sum',\n",
        "            'DC_POWER_mean_mean': 'DC_POWER_mean',\n",
        "            'AC_POWER_mean_mean': 'AC_POWER_mean',\n",
        "            'DAILY_YIELD_mean_mean': 'DAILY_YIELD',  # Simplified final name\n",
        "            'TOTAL_YIELD_mean_mean': 'TOTAL_YIELD'   # Simplified final name\n",
        "        }\n",
        "\n",
        "        # Rename columns for consistency\n",
        "        gen_agg = gen_agg.rename(columns=column_mapping)\n",
        "\n",
        "        # Debugging step: Print column names to check\n",
        "        print(\"\\nGeneration data columns after aggregation:\")\n",
        "        print(gen_agg.columns.tolist())\n",
        "\n",
        "        self.log_data_reduction(\"Generation Data Aggregation\", gen_agg, gen_before)\n",
        "\n",
        "        # 3. Process weather data with multiple interpolation methods\n",
        "        weather_before = self.weather_data.shape[0]\n",
        "\n",
        "        # Create different interpolated versions of weather data\n",
        "        weather_linear = self.weather_data.set_index('DATE_TIME').reindex(time_range)\n",
        "        weather_linear = weather_linear.interpolate(method='linear', limit_direction='both')\n",
        "\n",
        "        weather_cubic = self.weather_data.set_index('DATE_TIME').reindex(time_range)\n",
        "        weather_cubic = weather_cubic.interpolate(method='cubic', limit_direction='both')\n",
        "\n",
        "        # Combine interpolation methods based on gap size\n",
        "        weather_resampled = weather_cubic.copy()\n",
        "        large_gaps = weather_resampled.isna().any(axis=1)\n",
        "        weather_resampled[large_gaps] = weather_linear[large_gaps]\n",
        "\n",
        "        # Fill remaining gaps with forward/backward fill\n",
        "        weather_resampled = weather_resampled.fillna(method='ffill', limit=4)\n",
        "        weather_resampled = weather_resampled.fillna(method='bfill', limit=4)\n",
        "\n",
        "        weather_resampled = weather_resampled.reset_index()\n",
        "        weather_resampled = weather_resampled.rename(columns={'index': 'DATE_TIME'})\n",
        "\n",
        "        self.log_data_reduction(\"Weather Data Interpolation\", weather_resampled, weather_before)\n",
        "\n",
        "        # 4. Merge the generation and weather data\n",
        "        merged_before = gen_agg.shape[0]\n",
        "        merged = pd.merge(gen_agg, weather_resampled, on='DATE_TIME', how='inner')\n",
        "\n",
        "        # 5. Calculate efficiency metrics\n",
        "        merged['DC_AC_RATIO'] = merged['DC_POWER_sum'] / merged['AC_POWER_sum'].replace(0, np.nan)\n",
        "        merged['PERFORMANCE_RATIO'] = merged['DC_POWER_sum'] / merged['IRRADIATION'].replace(0, np.nan)\n",
        "\n",
        "        # 6. Handle missing values\n",
        "        merged = merged.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Interpolate missing numeric values first\n",
        "        numeric_columns = merged.select_dtypes(include=[np.number]).columns\n",
        "        merged[numeric_columns] = merged[numeric_columns].interpolate(method='linear', limit_direction='both', limit=4)\n",
        "\n",
        "        # Fill remaining gaps with forward/backward fill\n",
        "        merged = merged.fillna(method='ffill', limit=4)\n",
        "        merged = merged.fillna(method='bfill', limit=4)\n",
        "\n",
        "        # Drop rows where critical values are missing\n",
        "        critical_columns = ['DC_POWER_sum', 'AC_POWER_sum', 'IRRADIATION']\n",
        "        merged = merged.dropna(subset=critical_columns)\n",
        "\n",
        "        self.log_data_reduction(\"Final Merge and Cleaning\", merged, merged_before)\n",
        "\n",
        "        return merged\n",
        "\n",
        "    def engineer_features(self):\n",
        "        \"\"\"\n",
        "        Comprehensive feature engineering\n",
        "        \"\"\"\n",
        "        print(\"Starting feature engineering...\")\n",
        "\n",
        "        # Resample and merge data\n",
        "        self.merged_data = self._resample_and_align_data()\n",
        "\n",
        "        # Check data before feature engineering\n",
        "        print(f\"\\nShape before feature engineering: {self.merged_data.shape}\")\n",
        "\n",
        "        # Time-based features\n",
        "        self.merged_data['hour'] = self.merged_data['DATE_TIME'].dt.hour\n",
        "        self.merged_data['day'] = self.merged_data['DATE_TIME'].dt.day\n",
        "        self.merged_data['month'] = self.merged_data['DATE_TIME'].dt.month\n",
        "        self.merged_data['day_of_week'] = self.merged_data['DATE_TIME'].dt.dayofweek\n",
        "        self.merged_data['is_weekend'] = self.merged_data['day_of_week'].isin([5, 6]).astype(int)\n",
        "\n",
        "        # Solar position features\n",
        "        self.merged_data['day_of_year'] = self.merged_data['DATE_TIME'].dt.dayofyear\n",
        "        self.merged_data['solar_hour'] = np.sin(2 * np.pi * self.merged_data['hour'] / 24)\n",
        "        self.merged_data['solar_day'] = np.sin(2 * np.pi * self.merged_data['day_of_year'] / 365)\n",
        "\n",
        "        # Temperature features\n",
        "        self.merged_data['temp_diff'] = (self.merged_data['MODULE_TEMPERATURE'] -\n",
        "                                      self.merged_data['AMBIENT_TEMPERATURE'])\n",
        "\n",
        "        # Create backup of the data before lag features\n",
        "        data_before_lag = self.merged_data.copy()\n",
        "\n",
        "        # Lag features (multiple time steps)\n",
        "        for lag in [1, 2, 3, 12]:\n",
        "            self.merged_data[f'DC_POWER_sum_LAG_{lag}'] = self.merged_data['DC_POWER_sum'].shift(lag)\n",
        "            self.merged_data[f'AMBIENT_TEMPERATURE_LAG_{lag}'] = self.merged_data['AMBIENT_TEMPERATURE'].shift(lag)\n",
        "            self.merged_data[f'IRRADIATION_LAG_{lag}'] = self.merged_data['IRRADIATION'].shift(lag)\n",
        "\n",
        "        # Rolling statistics\n",
        "        for window in [4, 48]:\n",
        "            self.merged_data[f'DC_POWER_sum_ROLLING_MEAN_{window}'] = (\n",
        "                self.merged_data['DC_POWER_sum'].rolling(window=window, min_periods=1).mean())\n",
        "            self.merged_data[f'DC_POWER_sum_ROLLING_STD_{window}'] = (\n",
        "                self.merged_data['DC_POWER_sum'].rolling(window=window, min_periods=1).std())\n",
        "\n",
        "        # Handle missing values\n",
        "        self.merged_data = self.merged_data.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "        # Fill missing values with interpolation first\n",
        "        self.merged_data = self.merged_data.interpolate(method='linear', limit_direction='both', limit=4)\n",
        "\n",
        "        # Then use forward/backward fill for any remaining gaps\n",
        "        self.merged_data = self.merged_data.fillna(method='ffill')\n",
        "        self.merged_data = self.merged_data.fillna(method='bfill')\n",
        "\n",
        "        # Check if we lost all data\n",
        "        if len(self.merged_data) == 0:\n",
        "            print(\"Warning: All data was lost during feature engineering!\")\n",
        "            print(\"Restoring from backup and using simpler feature engineering...\")\n",
        "            self.merged_data = data_before_lag\n",
        "\n",
        "        print(\"Feature engineering complete.\")\n",
        "        print(f\"Final dataset shape: {self.merged_data.shape}\")\n",
        "\n",
        "        # Print column names for verification\n",
        "        print(\"\\nAvailable features:\")\n",
        "        for col in self.merged_data.columns:\n",
        "            print(f\"- {col}\")\n",
        "\n",
        "        return self.merged_data\n",
        "\n",
        "    def detect_anomalies(self):\n",
        "        \"\"\"\n",
        "        Detect anomalies in power generation\n",
        "        \"\"\"\n",
        "        # Isolation Forest for anomaly detection\n",
        "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "\n",
        "        # Features for anomaly detection\n",
        "        anomaly_features = ['DC_POWER_sum', 'AC_POWER_sum', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION', 'DC_AC_RATIO', 'PERFORMANCE_RATIO']\n",
        "\n",
        "        # Fit and predict\n",
        "        anomalies = iso_forest.fit_predict(self.merged_data[anomaly_features])\n",
        "        self.merged_data['is_anomaly'] = (anomalies == -1).astype(int)\n",
        "\n",
        "        # Calculate daily anomaly percentage\n",
        "        daily_anomalies = self.merged_data.groupby(\n",
        "            self.merged_data['DATE_TIME'].dt.dategenerate_eda_plots\n",
        "        )['is_anomaly'].mean() * 100\n",
        "\n",
        "        return daily_anomalies\n",
        "\n",
        "    def generate_eda_plots(self):\n",
        "        \"\"\" Generate comprehensive EDA plots with advanced analytics \"\"\"\n",
        "        plots = {}\n",
        "\n",
        "        # 1. Time Series Overview\n",
        "        fig, axes = plt.subplots(3, 1, figsize=(20, 15))\n",
        "        fig.suptitle('Power Generation Time Series Analysis', fontsize=16)\n",
        "\n",
        "        # Daily average\n",
        "        daily_power = self.merged_data.groupby(\n",
        "            self.merged_data['DATE_TIME'].dt.date)['DC_POWER_sum'].mean()\n",
        "        daily_power.plot(ax=axes[0], title='Daily Average DC Power')\n",
        "        axes[0].set_xlabel('Date')\n",
        "        axes[0].set_ylabel('DC Power (kW)')\n",
        "\n",
        "        # Weekly average with std\n",
        "        weekly_power = self.merged_data.groupby(\n",
        "            pd.Grouper(key='DATE_TIME', freq='W'))['DC_POWER_sum'].agg(['mean', 'std'])\n",
        "        weekly_power['mean'].plot(ax=axes[1], label='Mean')\n",
        "        axes[1].fill_between(weekly_power.index,\n",
        "                            weekly_power['mean'] - weekly_power['std'],\n",
        "                            weekly_power['mean'] + weekly_power['std'],\n",
        "                            alpha=0.3, label='±1 std')\n",
        "        axes[1].set_title('Weekly Average DC Power with Standard Deviation')\n",
        "        axes[1].set_xlabel('Week')\n",
        "        axes[1].set_ylabel('DC Power (kW)')\n",
        "        axes[1].legend()\n",
        "\n",
        "        # Hourly pattern\n",
        "        hourly_avg = self.merged_data.groupby('hour')['DC_POWER_sum'].mean()\n",
        "        hourly_std = self.merged_data.groupby('hour')['DC_POWER_sum'].std()\n",
        "        axes[2].plot(hourly_avg.index, hourly_avg.values, 'b-', label='Mean')\n",
        "        axes[2].fill_between(hourly_avg.index,\n",
        "                            hourly_avg - hourly_std,\n",
        "                            hourly_avg + hourly_std,\n",
        "                            alpha=0.3, label='±1 std')\n",
        "        axes[2].set_title('Average Daily Power Generation Pattern')\n",
        "        axes[2].set_xlabel('Hour of Day')\n",
        "        axes[2].set_ylabel('DC Power (kW)')\n",
        "        axes[2].legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plots['time_series_analysis'] = plt.gcf()\n",
        "        plt.close()\n",
        "\n",
        "        # 2. Correlation Analysis\n",
        "        plt.figure(figsize=(15, 12))\n",
        "\n",
        "        # Select important features\n",
        "        important_features = [\n",
        "            'DC_POWER_sum', 'AC_POWER_sum', 'AMBIENT_TEMPERATURE',\n",
        "            'MODULE_TEMPERATURE', 'IRRADIATION', 'DC_AC_RATIO',\n",
        "            'PERFORMANCE_RATIO', 'temp_diff',\n",
        "            'DC_POWER_sum_ROLLING_MEAN_4', 'DC_POWER_sum_ROLLING_STD_4'\n",
        "        ]\n",
        "\n",
        "        # Create correlation matrix\n",
        "        corr_matrix = self.merged_data[important_features].corr()\n",
        "\n",
        "        # Create mask for upper triangle\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "\n",
        "        # Create heatmap with annotations\n",
        "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm',\n",
        "                    fmt='.2f', square=True, linewidths=0.5)\n",
        "        plt.title('Feature Correlation Analysis', pad=20)\n",
        "        plots['correlation_analysis'] = plt.gcf()\n",
        "        plt.close()\n",
        "\n",
        "        # 3. Environmental Impact Analysis\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "        fig.suptitle('Environmental Factors Impact Analysis', fontsize=16)\n",
        "\n",
        "        # Temperature Impact\n",
        "        sns.scatterplot(data=self.merged_data,\n",
        "                      x='AMBIENT_TEMPERATURE',\n",
        "                      y='DC_POWER_sum',\n",
        "                      alpha=0.5, ax=axes[0, 0])\n",
        "        axes[0, 0].set_title('Temperature vs Power Generation')\n",
        "\n",
        "        # Irradiation Impact\n",
        "        sns.scatterplot(data=self.merged_data,\n",
        "                      x='IRRADIATION',\n",
        "                      y='DC_POWER_sum',\n",
        "                      alpha=0.5, ax=axes[0, 1])\n",
        "        axes[0, 1].set_title('Irradiation vs Power Generation')\n",
        "\n",
        "        # Efficiency Analysis\n",
        "        sns.boxplot(data=self.merged_data,\n",
        "                    x='hour',\n",
        "                    y='PERFORMANCE_RATIO',\n",
        "                    ax=axes[1, 0])\n",
        "        axes[1, 0].set_title('Performance Ratio by Hour')\n",
        "\n",
        "        # Temperature Difference Impact\n",
        "        sns.scatterplot(data=self.merged_data,\n",
        "                      x='temp_diff',\n",
        "                      y='DC_POWER_sum',\n",
        "                      alpha=0.5, ax=axes[1, 1])\n",
        "        axes[1, 1].set_title('Temperature Difference vs Power Generation')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plots['environmental_analysis'] = plt.gcf()\n",
        "        plt.close()\n",
        "\n",
        "        # 4. Performance Metrics Distribution\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "        fig.suptitle('Performance Metrics Distribution Analysis', fontsize=16)\n",
        "\n",
        "        # DC Power Distribution\n",
        "        sns.histplot(data=self.merged_data, x='DC_POWER_sum',\n",
        "                    kde=True, ax=axes[0, 0])\n",
        "        axes[0, 0].set_title('DC Power Distribution')\n",
        "\n",
        "        # AC Power Distribution\n",
        "        sns.histplot(data=self.merged_data, x='AC_POWER_sum',\n",
        "                    kde=True, ax=axes[0, 1])\n",
        "        axes[0, 1].set_title('AC Power Distribution')\n",
        "\n",
        "        # Performance Ratio Distribution\n",
        "        sns.histplot(data=self.merged_data, x='PERFORMANCE_RATIO',\n",
        "                    kde=True, ax=axes[1, 0])\n",
        "        axes[1, 0].set_title('Performance Ratio Distribution')\n",
        "\n",
        "        # DC/AC Ratio Distribution\n",
        "        sns.histplot(data=self.merged_data, x='DC_AC_RATIO',\n",
        "                    kde=True, ax=axes[1, 1])\n",
        "        axes[1, 1].set_title('DC/AC Ratio Distribution')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plots['distribution_analysis'] = plt.gcf()\n",
        "        plt.close()\n",
        "\n",
        "        # 5. Rolling Statistics Analysis\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(20, 12))\n",
        "        fig.suptitle('Rolling Statistics Analysis', fontsize=16)\n",
        "\n",
        "        # Plot rolling mean\n",
        "        self.merged_data.set_index('DATE_TIME')[['DC_POWER_sum',\n",
        "                                              'DC_POWER_sum_ROLLING_MEAN_48']].plot(ax=axes[0])\n",
        "        axes[0].set_title('DC Power vs 48-period Rolling Mean')\n",
        "        axes[0].set_xlabel('Date')\n",
        "        axes[0].set_ylabel('Power (kW)')\n",
        "\n",
        "        # Plot rolling standard deviation\n",
        "        self.merged_data['DC_POWER_sum_ROLLING_STD_48'].plot(ax=axes[1])\n",
        "        axes[1].set_title('48-period Rolling Standard Deviation')\n",
        "        axes[1].set_xlabel('Date')\n",
        "        axes[1].set_ylabel('Standard Deviation')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plots['rolling_statistics'] = plt.gcf()\n",
        "        plt.close()\n",
        "\n",
        "        return plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJKJKeus6R6X"
      },
      "outputs": [],
      "source": [
        "class SolarPowerModeling:\n",
        "    def __init__(self, data_processor):\n",
        "        self.data_processor = data_processor\n",
        "        self.models = {}\n",
        "        self.predictions = {}\n",
        "        self.feature_importance = {}\n",
        "        self.cv_results = {}\n",
        "        self.training_logs = {}\n",
        "\n",
        "    def log_training_step(self, step_name, details):\n",
        "        \"\"\"\n",
        "        Log training progress and parameters\n",
        "        \"\"\"\n",
        "        self.training_logs[step_name] = details\n",
        "        print(f\"\\n{step_name}:\")\n",
        "        for key, value in details.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "    def prepare_data(self, test_size=0.15, validation_size=0.15):\n",
        "        \"\"\"\n",
        "        Enhanced data preparation with better feature selection\n",
        "        \"\"\"\n",
        "        # Verify data exists and is not empty\n",
        "        if self.data_processor.merged_data is None or len(self.data_processor.merged_data) == 0:\n",
        "            raise ValueError(\"No data available for model training\")\n",
        "\n",
        "        print(\"\\nPreparing data for modeling...\")\n",
        "        print(f\"Total samples available: {len(self.data_processor.merged_data)}\")\n",
        "\n",
        "        # Print all available columns\n",
        "        print(\"\\nAll available columns:\")\n",
        "        print(self.data_processor.merged_data.columns.tolist())\n",
        "\n",
        "        # Identify numeric columns\n",
        "        numeric_cols = self.data_processor.merged_data.select_dtypes(\n",
        "            include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "        # Updated exclude cols\n",
        "        exclude_cols = [\n",
        "            'DAILY_YIELD', 'TOTAL_YIELD',  # Updated names\n",
        "            'PLANT_ID', 'DATE_TIME', 'SOURCE_KEY'\n",
        "        ]\n",
        "\n",
        "        # Print columns being excluded\n",
        "        print(\"\\nColumns being excluded:\")\n",
        "        print(exclude_cols)\n",
        "\n",
        "        # Create feature columns list\n",
        "        self.feature_cols = [col for col in numeric_cols\n",
        "                            if col not in exclude_cols]\n",
        "\n",
        "        # Print selected features\n",
        "        print(\"\\nSelected features:\")\n",
        "        print(self.feature_cols)\n",
        "\n",
        "        # Verify target column exists\n",
        "        if 'DC_POWER_sum' not in self.data_processor.merged_data.columns:\n",
        "            raise KeyError(\"Target column 'DC_POWER_sum' not found in dataset\")\n",
        "\n",
        "        # Use DC_POWER_sum as target variable\n",
        "        X = self.data_processor.merged_data[self.feature_cols]\n",
        "        y = self.data_processor.merged_data['DC_POWER_sum']\n",
        "\n",
        "        # Split data chronologically\n",
        "        train_size = 1 - (test_size + validation_size)\n",
        "        train_end = int(len(X) * train_size)\n",
        "        val_end = int(len(X) * (train_size + validation_size))\n",
        "\n",
        "        X_train = X[:train_end]\n",
        "        y_train = y[:train_end]\n",
        "        X_val = X[train_end:val_end]\n",
        "        y_val = y[train_end:val_end]\n",
        "        X_test = X[val_end:]\n",
        "        y_test = y[val_end:]\n",
        "\n",
        "        # Scale features using RobustScaler\n",
        "        scaler = RobustScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled = scaler.transform(X_val)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # Convert to DataFrames with feature names\n",
        "        X_train_scaled = pd.DataFrame(X_train_scaled, columns=self.feature_cols)\n",
        "        X_val_scaled = pd.DataFrame(X_val_scaled, columns=self.feature_cols)\n",
        "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=self.feature_cols)\n",
        "\n",
        "        # Store the data\n",
        "        self.train_data = (X_train_scaled, y_train)\n",
        "        self.val_data = (X_val_scaled, y_val)\n",
        "        self.test_data = (X_test_scaled, y_test)\n",
        "        self.scaler = scaler\n",
        "\n",
        "        # Log data preparation details\n",
        "        print(\"\\nData Preparation Details:\")\n",
        "        print(f\"Training set size: {X_train_scaled.shape}\")\n",
        "        print(f\"Validation set size: {X_val_scaled.shape}\")\n",
        "        print(f\"Test set size: {X_test_scaled.shape}\")\n",
        "        print(f\"\\nNumber of features: {len(self.feature_cols)}\")\n",
        "        print(\"\\nSelected features:\")\n",
        "        for col in self.feature_cols:\n",
        "            print(f\"- {col}\")\n",
        "\n",
        "        # Verify split sizes\n",
        "        print(\"\\nVerifying data splits:\")\n",
        "        print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "        print(f\"Validation samples: {len(X_val)} ({len(X_val)/len(X)*100:.1f}%)\")\n",
        "        print(f\"Test samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "        if len(X_train) == 0 or len(X_val) == 0 or len(X_test) == 0:\n",
        "            raise ValueError(\"One or more data splits are empty\")\n",
        "\n",
        "        return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
        "\n",
        "    def train_random_forest(self):\n",
        "        \"\"\"\n",
        "        Enhanced Random Forest training with more comprehensive hyperparameter tuning\n",
        "        \"\"\"\n",
        "        print(\"\\nTraining Random Forest model...\")\n",
        "\n",
        "        # Define enhanced parameter grid\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [10, 15, 20, None],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'max_features': ['sqrt', 'log2', None]\n",
        "        }\n",
        "\n",
        "        # Initialize base model\n",
        "        rf = RandomForestRegressor(\n",
        "            random_state=42,\n",
        "            n_jobs=-1,  # Use all available cores\n",
        "            oob_score=True  # Enable out-of-bag score estimation\n",
        "        )\n",
        "\n",
        "        # Configure TimeSeriesSplit with appropriate parameters\n",
        "        n_splits = 3  # Reduced from 5 to 3\n",
        "        test_size = int(len(self.train_data[0]) * 0.2)\n",
        "        tscv = TimeSeriesSplit(\n",
        "            n_splits=n_splits,\n",
        "            test_size=test_size,\n",
        "            gap=0\n",
        "        )\n",
        "\n",
        "        # Print split information\n",
        "        print(f\"\\nTime Series Split Configuration:\")\n",
        "        print(f\"Number of splits: {n_splits}\")\n",
        "        print(f\"Test size: {test_size}\")\n",
        "        print(f\"Training data size: {len(self.train_data[0])}\")\n",
        "\n",
        "        # Perform randomized search\n",
        "        random_search = RandomizedSearchCV(\n",
        "            estimator=rf,\n",
        "            param_distributions=param_grid,\n",
        "            n_iter=20,  # Number of parameter settings sampled\n",
        "            cv=tscv,\n",
        "            scoring=['neg_mean_squared_error', 'r2'],\n",
        "            refit='neg_mean_squared_error',\n",
        "            n_jobs=-1,\n",
        "            verbose=1,\n",
        "            random_state=42,\n",
        "            return_train_score=True\n",
        "        )\n",
        "\n",
        "        # Fit the model\n",
        "        X_train, y_train = self.train_data\n",
        "        random_search.fit(X_train, y_train)\n",
        "\n",
        "        # Store results\n",
        "        self.models['rf'] = random_search.best_estimator_\n",
        "        self.cv_results['rf'] = random_search.cv_results_\n",
        "\n",
        "        # Calculate and store feature importance\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': self.feature_cols,\n",
        "            'importance': random_search.best_estimator_.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        self.feature_importance['rf'] = feature_importance\n",
        "\n",
        "        # Log training results\n",
        "        self.log_training_step(\"Random Forest Training\", {\n",
        "            'Best parameters': random_search.best_params_,\n",
        "            'Best CV score': -random_search.best_score_,  # Convert back from negative MSE\n",
        "            'OOB score': random_search.best_estimator_.oob_score_,\n",
        "            'Top 5 features': feature_importance.head().to_dict(),\n",
        "            'Training duration': 'Completed'\n",
        "        })\n",
        "\n",
        "    def train_xgboost(self):\n",
        "        \"\"\"\n",
        "        Enhanced XGBoost training with comprehensive hyperparameter tuning\n",
        "        \"\"\"\n",
        "        print(\"\\nTraining XGBoost model...\")\n",
        "\n",
        "        # Define enhanced parameter grid\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'max_depth': [4, 6, 8, 10],\n",
        "            'learning_rate': [0.01, 0.05, 0.1],\n",
        "            'subsample': [0.8, 0.9, 1.0],\n",
        "            'colsample_bytree': [0.8, 0.9, 1.0],\n",
        "            'min_child_weight': [1, 3, 5],\n",
        "            'gamma': [0, 0.1, 0.2]\n",
        "        }\n",
        "\n",
        "        # Initialize base model\n",
        "        xgb = XGBRegressor(\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            early_stopping_rounds=20\n",
        "        )\n",
        "\n",
        "        # Configure TimeSeriesSplit with appropriate parameters\n",
        "        n_splits = 3  # Reduced from 5 to 3 to match Random Forest\n",
        "        test_size = int(len(self.train_data[0]) * 0.2)\n",
        "        tscv = TimeSeriesSplit(\n",
        "            n_splits=n_splits,\n",
        "            test_size=test_size,\n",
        "            gap=0\n",
        "        )\n",
        "\n",
        "        # Print split information\n",
        "        print(f\"\\nTime Series Split Configuration:\")\n",
        "        print(f\"Number of splits: {n_splits}\")\n",
        "        print(f\"Test size: {test_size}\")\n",
        "        print(f\"Training data size: {len(self.train_data[0])}\")\n",
        "\n",
        "        # Perform randomized search\n",
        "        random_search = RandomizedSearchCV(\n",
        "            estimator=xgb,\n",
        "            param_distributions=param_grid,\n",
        "            n_iter=20,\n",
        "            cv=tscv,\n",
        "            scoring=['neg_mean_squared_error', 'r2'],\n",
        "            refit='neg_mean_squared_error',\n",
        "            n_jobs=-1,\n",
        "            verbose=1,\n",
        "            random_state=42,\n",
        "            return_train_score=True\n",
        "        )\n",
        "\n",
        "        # Fit the model with validation data\n",
        "        X_train, y_train = self.train_data\n",
        "        X_val, y_val = self.val_data\n",
        "\n",
        "        random_search.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # Store results\n",
        "        self.models['xgb'] = random_search.best_estimator_\n",
        "        self.cv_results['xgb'] = random_search.cv_results_\n",
        "\n",
        "        # Calculate and store feature importance\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': self.feature_cols,\n",
        "            'importance': random_search.best_estimator_.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        self.feature_importance['xgb'] = feature_importance\n",
        "\n",
        "        # Log training results\n",
        "        self.log_training_step(\"XGBoost Training\", {\n",
        "            'Best parameters': random_search.best_params_,\n",
        "            'Best CV score': -random_search.best_score_,\n",
        "            'Top 5 features': feature_importance.head().to_dict(),\n",
        "            'Training duration': 'Completed'\n",
        "        })\n",
        "\n",
        "    def prepare_sequence_data(self, X, y, seq_length=6):\n",
        "        \"\"\"\n",
        "        Enhanced sequence data preparation with validation\n",
        "        \"\"\"\n",
        "        if len(X) < seq_length + 1:\n",
        "            raise ValueError(f\"Not enough data points. Have {len(X)}, need at least {seq_length + 1}\")\n",
        "\n",
        "        # Convert DataFrame to numpy array if necessary\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        if isinstance(y, pd.Series):\n",
        "            y = y.values\n",
        "\n",
        "        X_seq, y_seq = [], []\n",
        "\n",
        "        # Create sequences with overlap\n",
        "        for i in range(len(X) - seq_length):\n",
        "            X_seq.append(X[i:(i + seq_length)])\n",
        "            y_seq.append(y[i + seq_length])\n",
        "\n",
        "        return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "    def train_lstm(self):\n",
        "        \"\"\"\n",
        "        Enhanced LSTM training with improved architecture and training process\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"\\nTraining LSTM model...\")\n",
        "\n",
        "            # Get training and validation data\n",
        "            X_train, y_train = self.train_data\n",
        "            X_val, y_val = self.val_data\n",
        "\n",
        "            # Initialize scalers\n",
        "            feature_scaler = RobustScaler()  # Changed to RobustScaler for better handling of outliers\n",
        "            target_scaler = RobustScaler()\n",
        "\n",
        "            # Handle NaN and negative values with more careful preprocessing\n",
        "            X_train = np.clip(X_train, 0, None)\n",
        "            X_val = np.clip(X_val, 0, None)\n",
        "            y_train = np.clip(y_train, 0, None)\n",
        "            y_val = np.clip(y_val, 0, None)\n",
        "\n",
        "            # Handle outliers before scaling\n",
        "            def remove_outliers(data, threshold=3):\n",
        "                z_scores = np.abs((data - np.mean(data)) / np.std(data))\n",
        "                return np.where(z_scores > threshold, np.mean(data), data)\n",
        "\n",
        "            X_train = np.apply_along_axis(remove_outliers, 0, X_train)\n",
        "            X_val = np.apply_along_axis(remove_outliers, 0, X_val)\n",
        "\n",
        "            # Scale features\n",
        "            X_train_scaled = feature_scaler.fit_transform(X_train)\n",
        "            X_val_scaled = feature_scaler.transform(X_val)\n",
        "\n",
        "            # Scale target\n",
        "            y_train_scaled = target_scaler.fit_transform(y_train.values.reshape(-1, 1)).flatten()\n",
        "            y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "            # Create sequences with optimal length\n",
        "            seq_length = 16  # Increased sequence length\n",
        "            stride = 1  # Reduced stride for more training samples\n",
        "\n",
        "            X_train_seq, y_train_seq = [], []\n",
        "            X_val_seq, y_val_seq = [], []\n",
        "\n",
        "            # Prepare sequences\n",
        "            for i in range(0, len(X_train_scaled) - seq_length, stride):\n",
        "                X_train_seq.append(X_train_scaled[i:i + seq_length])\n",
        "                y_train_seq.append(y_train_scaled[i + seq_length])\n",
        "\n",
        "            for i in range(0, len(X_val_scaled) - seq_length, stride):\n",
        "                X_val_seq.append(X_val_scaled[i:i + seq_length])\n",
        "                y_val_seq.append(y_val_scaled[i + seq_length])\n",
        "\n",
        "            X_train_seq = np.array(X_train_seq)\n",
        "            y_train_seq = np.array(y_train_seq)\n",
        "            X_val_seq = np.array(X_val_seq)\n",
        "            y_val_seq = np.array(y_val_seq)\n",
        "\n",
        "            print(f\"Training sequence shape: {X_train_seq.shape}\")\n",
        "            print(f\"Validation sequence shape: {X_val_seq.shape}\")\n",
        "\n",
        "            # Build enhanced LSTM model\n",
        "            model = Sequential([\n",
        "                # First LSTM layer with increased capacity\n",
        "                LSTM(128, input_shape=(seq_length, X_train.shape[1]),\n",
        "                    return_sequences=True,\n",
        "                    kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.3),\n",
        "\n",
        "                # Second LSTM layer\n",
        "                LSTM(64, return_sequences=True,\n",
        "                    kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.3),\n",
        "\n",
        "                # Third LSTM layer\n",
        "                LSTM(32, return_sequences=False,\n",
        "                    kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
        "                BatchNormalization(),\n",
        "                Dropout(0.3),\n",
        "\n",
        "                # Dense layers with skip connections\n",
        "                Dense(32, activation='relu'),\n",
        "                BatchNormalization(),\n",
        "                Dense(16, activation='relu'),\n",
        "                Dropout(0.2),\n",
        "\n",
        "                # Output layer\n",
        "                Dense(1, activation='relu')\n",
        "            ])\n",
        "\n",
        "            # Custom loss function combining Huber and MSE\n",
        "            def custom_loss(y_true, y_pred):\n",
        "                huber = tf.keras.losses.Huber(delta=1.0)(y_true, y_pred)\n",
        "                mse = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n",
        "                return 0.7 * huber + 0.3 * mse\n",
        "\n",
        "            # Initialize optimizer with gradient clipping\n",
        "            optimizer = Adam(\n",
        "                learning_rate=0.0005,  # Lower initial learning rate\n",
        "                clipnorm=1.0,\n",
        "                beta_1=0.9,\n",
        "                beta_2=0.999,\n",
        "                epsilon=1e-07\n",
        "            )\n",
        "\n",
        "            model.compile(\n",
        "                optimizer=optimizer,\n",
        "                loss=custom_loss,\n",
        "                metrics=['mae', 'mse']\n",
        "            )\n",
        "\n",
        "            # Print model summary\n",
        "            model.summary()\n",
        "\n",
        "            # Enhanced callbacks\n",
        "            callbacks = [\n",
        "                EarlyStopping(\n",
        "                    monitor='val_loss',\n",
        "                    patience=15,  # Increased patience\n",
        "                    restore_best_weights=True,\n",
        "                    min_delta=0.0001\n",
        "                ),\n",
        "                ReduceLROnPlateau(\n",
        "                    monitor='val_loss',\n",
        "                    factor=0.2,  # More aggressive reduction\n",
        "                    patience=7,\n",
        "                    min_lr=0.00001,\n",
        "                    verbose=1\n",
        "                ),\n",
        "                ModelCheckpoint(\n",
        "                    'best_lstm_model.keras',\n",
        "                    monitor='val_loss',\n",
        "                    save_best_only=True,\n",
        "                    verbose=1\n",
        "                )\n",
        "            ]\n",
        "\n",
        "            # Train with modified parameters\n",
        "            history = model.fit(\n",
        "                X_train_seq, y_train_seq,\n",
        "                validation_data=(X_val_seq, y_val_seq),\n",
        "                epochs=50,  # Increased epochs\n",
        "                batch_size=64,  # Increased batch size\n",
        "                callbacks=callbacks,\n",
        "                shuffle=False,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Store model and scalers\n",
        "            self.models['lstm'] = model\n",
        "            self.scalers = {\n",
        "                'feature_scaler': feature_scaler,\n",
        "                'target_scaler': target_scaler,\n",
        "                'seq_length': seq_length\n",
        "            }\n",
        "\n",
        "            return model, history\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in LSTM training: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return None, None\n",
        "\n",
        "    def create_ensemble(self):\n",
        "        \"\"\"\n",
        "        Fixed ensemble creation with proper error handling\n",
        "        \"\"\"\n",
        "        print(\"\\nCreating ensemble model...\")\n",
        "\n",
        "        try:\n",
        "            # Get available models\n",
        "            available_models = {name: model for name, model in self.models.items()\n",
        "                              if model is not None}\n",
        "\n",
        "            if len(available_models) < 2:\n",
        "                print(\"Not enough models available for ensemble creation\")\n",
        "                return None\n",
        "\n",
        "            # Calculate weights based on validation performance\n",
        "            weights = {}\n",
        "            X_val, y_val = self.val_data\n",
        "\n",
        "            for name, model in available_models.items():\n",
        "                try:\n",
        "                    if name == 'lstm':\n",
        "                        # Handle LSTM predictions separately\n",
        "                        seq_length = self.scalers['seq_length']\n",
        "                        X_val_scaled = self.scalers['feature_scaler'].transform(X_val)\n",
        "                        X_val_seq = np.array([X_val_scaled[i:i + seq_length]\n",
        "                                            for i in range(len(X_val_scaled) - seq_length)])\n",
        "                        val_pred = model.predict(X_val_seq).flatten()\n",
        "                        val_score = r2_score(y_val[seq_length:], val_pred)\n",
        "                    else:\n",
        "                        # Handle RF and XGB predictions\n",
        "                        val_pred = model.predict(X_val)\n",
        "                        val_score = r2_score(y_val, val_pred)\n",
        "\n",
        "                    weights[name] = max(val_score, 0)  # Ensure non-negative weights\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error calculating weight for {name}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            # Normalize weights\n",
        "            if weights:\n",
        "                total = sum(weights.values())\n",
        "                if total > 0:\n",
        "                    self.ensemble_weights = {k: v/total for k, v in weights.items()}\n",
        "                    print(\"Ensemble Creation:\")\n",
        "                    print(f\"Model weights: {self.ensemble_weights}\")\n",
        "                    print(f\"Number of models: {len(self.ensemble_weights)}\")\n",
        "                    print(f\"Included models: {list(self.ensemble_weights.keys())}\")\n",
        "                else:\n",
        "                    print(\"All model weights were zero\")\n",
        "                    self.ensemble_weights = None\n",
        "            else:\n",
        "                print(\"No valid models for ensemble creation\")\n",
        "                self.ensemble_weights = None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ensemble creation: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            self.ensemble_weights = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UpAj0T086Vxq",
        "outputId": "921dac8b-2bc0-4812-8f99-2008c29cc492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Initial Generation Data Load:\n",
            "Previous rows: N/A\n",
            "Current rows: 68778\n",
            "\n",
            "Initial Weather Data Load:\n",
            "Previous rows: N/A\n",
            "Current rows: 3182\n",
            "\n",
            "Generation data frequency: 0 days 00:00:00\n",
            "Weather data frequency: 0 days 00:15:00\n",
            "Starting feature engineering...\n",
            "\n",
            "Starting data resampling and alignment...\n",
            "\n",
            "Generation data columns after aggregation:\n",
            "['DATE_TIME', 'DC_POWER_sum', 'DC_POWER_mean', 'DC_POWER_mean_std', 'DC_POWER_mean_min', 'DC_POWER_mean_max', 'AC_POWER_sum', 'AC_POWER_mean', 'AC_POWER_mean_std', 'AC_POWER_mean_min', 'AC_POWER_mean_max', 'DC_POWER_std_mean', 'DC_POWER_min_min', 'DC_POWER_max_max', 'AC_POWER_std_mean', 'AC_POWER_min_min', 'AC_POWER_max_max', 'DAILY_YIELD', 'TOTAL_YIELD']\n",
            "\n",
            "Generation Data Aggregation:\n",
            "Previous rows: 68778\n",
            "Current rows: 3158\n",
            "Reduction: 65620 rows (95.41%)\n",
            "\n",
            "Weather Data Interpolation:\n",
            "Previous rows: 3182\n",
            "Current rows: 3264\n",
            "Reduction: -82 rows (-2.58%)\n",
            "\n",
            "Final Merge and Cleaning:\n",
            "Previous rows: 3158\n",
            "Current rows: 3158\n",
            "Reduction: 0 rows (0.00%)\n",
            "\n",
            "Shape before feature engineering: (3158, 26)\n",
            "Feature engineering complete.\n",
            "Final dataset shape: (3158, 51)\n",
            "\n",
            "Available features:\n",
            "- DATE_TIME\n",
            "- DC_POWER_sum\n",
            "- DC_POWER_mean\n",
            "- DC_POWER_mean_std\n",
            "- DC_POWER_mean_min\n",
            "- DC_POWER_mean_max\n",
            "- AC_POWER_sum\n",
            "- AC_POWER_mean\n",
            "- AC_POWER_mean_std\n",
            "- AC_POWER_mean_min\n",
            "- AC_POWER_mean_max\n",
            "- DC_POWER_std_mean\n",
            "- DC_POWER_min_min\n",
            "- DC_POWER_max_max\n",
            "- AC_POWER_std_mean\n",
            "- AC_POWER_min_min\n",
            "- AC_POWER_max_max\n",
            "- DAILY_YIELD\n",
            "- TOTAL_YIELD\n",
            "- PLANT_ID\n",
            "- SOURCE_KEY\n",
            "- AMBIENT_TEMPERATURE\n",
            "- MODULE_TEMPERATURE\n",
            "- IRRADIATION\n",
            "- DC_AC_RATIO\n",
            "- PERFORMANCE_RATIO\n",
            "- hour\n",
            "- day\n",
            "- month\n",
            "- day_of_week\n",
            "- is_weekend\n",
            "- day_of_year\n",
            "- solar_hour\n",
            "- solar_day\n",
            "- temp_diff\n",
            "- DC_POWER_sum_LAG_1\n",
            "- AMBIENT_TEMPERATURE_LAG_1\n",
            "- IRRADIATION_LAG_1\n",
            "- DC_POWER_sum_LAG_2\n",
            "- AMBIENT_TEMPERATURE_LAG_2\n",
            "- IRRADIATION_LAG_2\n",
            "- DC_POWER_sum_LAG_3\n",
            "- AMBIENT_TEMPERATURE_LAG_3\n",
            "- IRRADIATION_LAG_3\n",
            "- DC_POWER_sum_LAG_12\n",
            "- AMBIENT_TEMPERATURE_LAG_12\n",
            "- IRRADIATION_LAG_12\n",
            "- DC_POWER_sum_ROLLING_MEAN_4\n",
            "- DC_POWER_sum_ROLLING_STD_4\n",
            "- DC_POWER_sum_ROLLING_MEAN_48\n",
            "- DC_POWER_sum_ROLLING_STD_48\n",
            "Generated EDA plots successfully\n",
            "\n",
            "Preparing data for modeling...\n",
            "Total samples available: 3158\n",
            "\n",
            "All available columns:\n",
            "['DATE_TIME', 'DC_POWER_sum', 'DC_POWER_mean', 'DC_POWER_mean_std', 'DC_POWER_mean_min', 'DC_POWER_mean_max', 'AC_POWER_sum', 'AC_POWER_mean', 'AC_POWER_mean_std', 'AC_POWER_mean_min', 'AC_POWER_mean_max', 'DC_POWER_std_mean', 'DC_POWER_min_min', 'DC_POWER_max_max', 'AC_POWER_std_mean', 'AC_POWER_min_min', 'AC_POWER_max_max', 'DAILY_YIELD', 'TOTAL_YIELD', 'PLANT_ID', 'SOURCE_KEY', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION', 'DC_AC_RATIO', 'PERFORMANCE_RATIO', 'hour', 'day', 'month', 'day_of_week', 'is_weekend', 'day_of_year', 'solar_hour', 'solar_day', 'temp_diff', 'DC_POWER_sum_LAG_1', 'AMBIENT_TEMPERATURE_LAG_1', 'IRRADIATION_LAG_1', 'DC_POWER_sum_LAG_2', 'AMBIENT_TEMPERATURE_LAG_2', 'IRRADIATION_LAG_2', 'DC_POWER_sum_LAG_3', 'AMBIENT_TEMPERATURE_LAG_3', 'IRRADIATION_LAG_3', 'DC_POWER_sum_LAG_12', 'AMBIENT_TEMPERATURE_LAG_12', 'IRRADIATION_LAG_12', 'DC_POWER_sum_ROLLING_MEAN_4', 'DC_POWER_sum_ROLLING_STD_4', 'DC_POWER_sum_ROLLING_MEAN_48', 'DC_POWER_sum_ROLLING_STD_48']\n",
            "\n",
            "Columns being excluded:\n",
            "['DAILY_YIELD', 'TOTAL_YIELD', 'PLANT_ID', 'DATE_TIME', 'SOURCE_KEY']\n",
            "\n",
            "Selected features:\n",
            "['DC_POWER_sum', 'DC_POWER_mean', 'DC_POWER_mean_std', 'DC_POWER_mean_min', 'DC_POWER_mean_max', 'AC_POWER_sum', 'AC_POWER_mean', 'AC_POWER_mean_std', 'AC_POWER_mean_min', 'AC_POWER_mean_max', 'DC_POWER_std_mean', 'DC_POWER_min_min', 'DC_POWER_max_max', 'AC_POWER_std_mean', 'AC_POWER_min_min', 'AC_POWER_max_max', 'AMBIENT_TEMPERATURE', 'MODULE_TEMPERATURE', 'IRRADIATION', 'DC_AC_RATIO', 'PERFORMANCE_RATIO', 'is_weekend', 'solar_hour', 'solar_day', 'temp_diff', 'DC_POWER_sum_LAG_1', 'AMBIENT_TEMPERATURE_LAG_1', 'IRRADIATION_LAG_1', 'DC_POWER_sum_LAG_2', 'AMBIENT_TEMPERATURE_LAG_2', 'IRRADIATION_LAG_2', 'DC_POWER_sum_LAG_3', 'AMBIENT_TEMPERATURE_LAG_3', 'IRRADIATION_LAG_3', 'DC_POWER_sum_LAG_12', 'AMBIENT_TEMPERATURE_LAG_12', 'IRRADIATION_LAG_12', 'DC_POWER_sum_ROLLING_MEAN_4', 'DC_POWER_sum_ROLLING_STD_4', 'DC_POWER_sum_ROLLING_MEAN_48', 'DC_POWER_sum_ROLLING_STD_48']\n",
            "\n",
            "Data Preparation Details:\n",
            "Training set size: (2210, 41)\n",
            "Validation set size: (474, 41)\n",
            "Test set size: (474, 41)\n",
            "\n",
            "Number of features: 41\n",
            "\n",
            "Selected features:\n",
            "- DC_POWER_sum\n",
            "- DC_POWER_mean\n",
            "- DC_POWER_mean_std\n",
            "- DC_POWER_mean_min\n",
            "- DC_POWER_mean_max\n",
            "- AC_POWER_sum\n",
            "- AC_POWER_mean\n",
            "- AC_POWER_mean_std\n",
            "- AC_POWER_mean_min\n",
            "- AC_POWER_mean_max\n",
            "- DC_POWER_std_mean\n",
            "- DC_POWER_min_min\n",
            "- DC_POWER_max_max\n",
            "- AC_POWER_std_mean\n",
            "- AC_POWER_min_min\n",
            "- AC_POWER_max_max\n",
            "- AMBIENT_TEMPERATURE\n",
            "- MODULE_TEMPERATURE\n",
            "- IRRADIATION\n",
            "- DC_AC_RATIO\n",
            "- PERFORMANCE_RATIO\n",
            "- is_weekend\n",
            "- solar_hour\n",
            "- solar_day\n",
            "- temp_diff\n",
            "- DC_POWER_sum_LAG_1\n",
            "- AMBIENT_TEMPERATURE_LAG_1\n",
            "- IRRADIATION_LAG_1\n",
            "- DC_POWER_sum_LAG_2\n",
            "- AMBIENT_TEMPERATURE_LAG_2\n",
            "- IRRADIATION_LAG_2\n",
            "- DC_POWER_sum_LAG_3\n",
            "- AMBIENT_TEMPERATURE_LAG_3\n",
            "- IRRADIATION_LAG_3\n",
            "- DC_POWER_sum_LAG_12\n",
            "- AMBIENT_TEMPERATURE_LAG_12\n",
            "- IRRADIATION_LAG_12\n",
            "- DC_POWER_sum_ROLLING_MEAN_4\n",
            "- DC_POWER_sum_ROLLING_STD_4\n",
            "- DC_POWER_sum_ROLLING_MEAN_48\n",
            "- DC_POWER_sum_ROLLING_STD_48\n",
            "\n",
            "Verifying data splits:\n",
            "Training samples: 2210 (70.0%)\n",
            "Validation samples: 474 (15.0%)\n",
            "Test samples: 474 (15.0%)\n",
            "\n",
            "Training Random Forest model...\n",
            "\n",
            "Time Series Split Configuration:\n",
            "Number of splits: 3\n",
            "Test size: 442\n",
            "Training data size: 2210\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "\n",
            "Random Forest Training:\n",
            "Best parameters: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': None, 'max_depth': 15}\n",
            "Best CV score: 2902723.0427758857\n",
            "OOB score: 0.9997971525906688\n",
            "Top 5 features: {'feature': {0: 'DC_POWER_sum', 5: 'AC_POWER_sum', 6: 'AC_POWER_mean', 1: 'DC_POWER_mean', 15: 'AC_POWER_max_max'}, 'importance': {0: 0.4094988973593447, 5: 0.3752037368924726, 6: 0.11905849468891891, 1: 0.09391796781170136, 15: 0.0004633837408863291}}\n",
            "Training duration: Completed\n",
            "\n",
            "Training XGBoost model...\n",
            "\n",
            "Time Series Split Configuration:\n",
            "Number of splits: 3\n",
            "Test size: 442\n",
            "Training data size: 2210\n",
            "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
            "\n",
            "XGBoost Training:\n",
            "Best parameters: {'subsample': 0.8, 'n_estimators': 200, 'min_child_weight': 1, 'max_depth': 8, 'learning_rate': 0.05, 'gamma': 0.1, 'colsample_bytree': 1.0}\n",
            "Best CV score: 526457.8123700854\n",
            "Top 5 features: {'feature': {5: 'AC_POWER_sum', 0: 'DC_POWER_sum', 1: 'DC_POWER_mean', 6: 'AC_POWER_mean', 3: 'DC_POWER_mean_min'}, 'importance': {5: 0.5946476459503174, 0: 0.35861045122146606, 1: 0.04189583286643028, 6: 0.0031128255650401115, 3: 0.0007555937627330422}}\n",
            "Training duration: Completed\n",
            "\n",
            "Training LSTM model...\n",
            "Training sequence shape: (2194, 16, 41)\n",
            "Validation sequence shape: (458, 16, 41)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m87,040\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │          \u001b[38;5;34m49,408\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │          \u001b[38;5;34m12,416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m1,056\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │             \u001b[38;5;34m528\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m17\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">87,040</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m151,489\u001b[0m (591.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">151,489</span> (591.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m150,977\u001b[0m (589.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">150,977</span> (589.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m512\u001b[0m (2.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> (2.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 1: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 146ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 5.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 2: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 5.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 3: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 5.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 4: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 5.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 5: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 5.0000e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 6: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 96ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 7: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 115ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 8: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 1.0000e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 9: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 1.0000e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 10: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 1.0000e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 11: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 1.0000e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 12: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 68ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 1.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 13: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 1.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 2.0000000949949027e-05.\n",
            "\n",
            "Epoch 14: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 65ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 1.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m34/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: nan - mae: nan - mse: nan\n",
            "Epoch 15: val_loss did not improve from inf\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 64ms/step - loss: nan - mae: nan - mse: nan - val_loss: nan - val_mae: nan - val_mse: nan - learning_rate: 2.0000e-05\n",
            "\n",
            "Creating ensemble model...\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 67ms/step\n",
            "Error calculating weight for lstm: Input contains NaN.\n",
            "Ensemble Creation:\n",
            "Model weights: {'rf': 0.5000032354187437, 'xgb': 0.49999676458125636}\n",
            "Number of models: 2\n",
            "Included models: ['rf', 'xgb']\n",
            "\n",
            "Evaluating models...\n",
            "Original test set length: 474\n",
            "Starting LSTM evaluation...\n",
            "Test sequence shapes - X: (458, 16, 41), y: (458,)\n",
            "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Final shapes - predictions: (458,), actual: (458,)\n",
            "Range - predictions: [nan, nan], actual: [0.00, 297928.39]\n",
            "Error in metric calculation: No valid data points for metric calculation\n",
            "LSTM metrics calculated successfully\n",
            "LSTM prediction length: 458\n",
            "LSTM evaluation complete\n",
            "Evaluating RF...\n",
            "RF evaluation complete\n",
            "Evaluating XGBoost...\n",
            "XGB evaluation complete\n",
            "Creating ensemble predictions...\n",
            "Ensemble evaluation complete\n",
            "Generated evaluation plots successfully\n",
            "\n",
            "Model Evaluation Results:\n",
            "\n",
            "LSTM Results:\n",
            "lstm_error: No valid data points for metric calculation\n",
            "\n",
            "RF Results:\n",
            "rf_RMSE: 338.5488\n",
            "rf_MAE: 112.2943\n",
            "rf_R2: 1.0000\n",
            "rf_Normalized_RMSE: 0.0050\n",
            "rf_MAPE: 0.5929\n",
            "rf_Bias: -14.1433\n",
            "rf_Max_Error: 6055.3692\n",
            "\n",
            "XGB Results:\n",
            "xgb_RMSE: 443.6670\n",
            "xgb_MAE: 191.7978\n",
            "xgb_R2: 1.0000\n",
            "xgb_Normalized_RMSE: 0.0066\n",
            "xgb_MAPE: 2.7028\n",
            "xgb_Bias: -79.0665\n",
            "xgb_Max_Error: 6615.2054\n",
            "\n",
            "ENSEMBLE Results:\n",
            "ensemble_RMSE: 341.2745\n",
            "ensemble_MAE: 106.3593\n",
            "ensemble_R2: 1.0000\n",
            "ensemble_Normalized_RMSE: 0.0051\n",
            "ensemble_MAPE: 1.4161\n",
            "ensemble_Bias: -46.6056\n",
            "ensemble_Max_Error: 6335.2961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved rf model\n",
            "Successfully saved xgb model\n",
            "Successfully saved lstm model\n",
            "Successfully saved plot: time_series_analysis\n",
            "Successfully saved plot: correlation_analysis\n",
            "Successfully saved plot: environmental_analysis\n",
            "Successfully saved plot: distribution_analysis\n",
            "Successfully saved plot: rolling_statistics\n",
            "Successfully saved plot: prediction_comparison\n",
            "Successfully saved plot: time_series\n",
            "Successfully saved evaluation results\n",
            "\n",
            "Execution complete. Results and plots have been saved.\n",
            "\n",
            "Best Model: rf\n",
            "\n",
            "Model Rankings:\n",
            "rf: RMSE = 338.5488\n",
            "ensemble: RMSE = 341.2745\n",
            "xgb: RMSE = 443.6670\n"
          ]
        }
      ],
      "source": [
        "class ModelEvaluation:\n",
        "    def __init__(self, model_trainer):\n",
        "        self.model_trainer = model_trainer\n",
        "        self.evaluation_results = {}\n",
        "        self.visualization_results = {}\n",
        "        self.error_analysis = {}\n",
        "        self.detailed_metrics = {}\n",
        "        # Add this line to get scalers from model trainer\n",
        "        self.scalers = getattr(self.model_trainer, 'scalers', None)\n",
        "\n",
        "    def calculate_metrics(self, y_true, y_pred, prefix=''):\n",
        "        \"\"\"\n",
        "        Comprehensive metric calculation\n",
        "        \"\"\"\n",
        "        metrics = {\n",
        "            f'{prefix}RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "            f'{prefix}MAE': mean_absolute_error(y_true, y_pred),\n",
        "            f'{prefix}R2': r2_score(y_true, y_pred),\n",
        "            f'{prefix}MAPE': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,\n",
        "            f'{prefix}Normalized_RMSE': np.sqrt(mean_squared_error(y_true, y_pred)) / np.mean(y_true),\n",
        "            f'{prefix}Bias': np.mean(y_pred - y_true),\n",
        "            f'{prefix}Max_Error': np.max(np.abs(y_pred - y_true))\n",
        "        }\n",
        "\n",
        "        # Add time-based metrics\n",
        "        if len(y_true) > 24:  # If we have at least a day's worth of data\n",
        "            # Daily metrics\n",
        "            daily_true = pd.Series(y_true).rolling(window=24).mean()\n",
        "            daily_pred = pd.Series(y_pred).rolling(window=24).mean()\n",
        "            metrics[f'{prefix}Daily_RMSE'] = np.sqrt(mean_squared_error(\n",
        "                daily_true.dropna(), daily_pred.dropna()))\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _calculate_metrics(self, y_true, y_pred, prefix=''):\n",
        "        \"\"\"\n",
        "        Enhanced metric calculation with better validation\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Ensure arrays are numpy arrays\n",
        "            y_true = np.asarray(y_true)\n",
        "            y_pred = np.asarray(y_pred)\n",
        "\n",
        "            # Remove any NaN values\n",
        "            mask = ~(np.isnan(y_true) | np.isnan(y_pred) |\n",
        "                    np.isinf(y_true) | np.isinf(y_pred))\n",
        "\n",
        "            y_true = y_true[mask]\n",
        "            y_pred = y_pred[mask]\n",
        "\n",
        "            if len(y_true) == 0 or len(y_pred) == 0:\n",
        "                raise ValueError(\"No valid data points for metric calculation\")\n",
        "\n",
        "            # Ensure same length\n",
        "            min_len = min(len(y_true), len(y_pred))\n",
        "            y_true = y_true[:min_len]\n",
        "            y_pred = y_pred[:min_len]\n",
        "\n",
        "            # Calculate basic metrics with error checking\n",
        "            metrics = {}\n",
        "\n",
        "            try:\n",
        "                metrics[f'{prefix}RMSE'] = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating RMSE: {e}\")\n",
        "\n",
        "            try:\n",
        "                metrics[f'{prefix}MAE'] = mean_absolute_error(y_true, y_pred)\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating MAE: {e}\")\n",
        "\n",
        "            try:\n",
        "                metrics[f'{prefix}R2'] = r2_score(y_true, y_pred)\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating R2: {e}\")\n",
        "\n",
        "            # Calculate normalized RMSE\n",
        "            try:\n",
        "                mean_true = np.mean(y_true)\n",
        "                if mean_true != 0:\n",
        "                    metrics[f'{prefix}Normalized_RMSE'] = metrics[f'{prefix}RMSE'] / mean_true\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating Normalized RMSE: {e}\")\n",
        "\n",
        "            # Calculate MAPE for non-zero values\n",
        "            try:\n",
        "                nonzero_mask = y_true != 0\n",
        "                if np.any(nonzero_mask):\n",
        "                    y_true_nz = y_true[nonzero_mask]\n",
        "                    y_pred_nz = y_pred[nonzero_mask]\n",
        "                    metrics[f'{prefix}MAPE'] = np.mean(np.abs((y_true_nz - y_pred_nz) / y_true_nz)) * 100\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating MAPE: {e}\")\n",
        "\n",
        "            # Additional metrics\n",
        "            try:\n",
        "                metrics[f'{prefix}Bias'] = np.mean(y_pred - y_true)\n",
        "                metrics[f'{prefix}Max_Error'] = np.max(np.abs(y_pred - y_true))\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating additional metrics: {e}\")\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in metric calculation: {str(e)}\")\n",
        "            return {f'{prefix}error': str(e)}\n",
        "\n",
        "    def evaluate_models(self):\n",
        "        \"\"\"\n",
        "        Fixed model evaluation with proper variable scope\n",
        "        \"\"\"\n",
        "        print(\"\\nEvaluating models...\")\n",
        "        self.evaluation_results = {}\n",
        "        self.detailed_metrics = {}\n",
        "        test_length = None  # Initialize test_length at the start\n",
        "\n",
        "        try:\n",
        "            X_test, y_test = self.model_trainer.test_data\n",
        "            original_length = len(y_test)\n",
        "            print(f\"Original test set length: {original_length}\")\n",
        "\n",
        "            # Evaluate LSTM first\n",
        "            if self.model_trainer.models.get('lstm') is not None:\n",
        "                try:\n",
        "                    print(\"Starting LSTM evaluation...\")\n",
        "                    lstm_metrics, lstm_pred, lstm_test = self.evaluate_lstm(X_test, y_test)\n",
        "                    if lstm_metrics is not None and lstm_pred is not None:\n",
        "                        test_length = len(lstm_pred)  # Set test_length from LSTM results\n",
        "                        print(f\"LSTM prediction length: {test_length}\")\n",
        "\n",
        "                        # Store LSTM results\n",
        "                        self.evaluation_results['lstm'] = lstm_metrics\n",
        "                        self.detailed_metrics['lstm'] = {\n",
        "                            'predictions': lstm_pred,\n",
        "                            'actual': lstm_test,\n",
        "                            'metrics': lstm_metrics\n",
        "                        }\n",
        "                        print(\"LSTM evaluation complete\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in LSTM evaluation: {str(e)}\")\n",
        "                    test_length = original_length  # Fall back to original length\n",
        "            else:\n",
        "                test_length = original_length\n",
        "\n",
        "            # Use test_length for other models\n",
        "            y_test_adjusted = y_test[-test_length:] if test_length else y_test\n",
        "            X_test_adjusted = X_test[-test_length:] if test_length else X_test\n",
        "\n",
        "            # Evaluate RF\n",
        "            if self.model_trainer.models.get('rf') is not None:\n",
        "                try:\n",
        "                    print(\"Evaluating RF...\")\n",
        "                    rf_pred = self.model_trainer.models['rf'].predict(X_test_adjusted)\n",
        "                    rf_metrics = self._calculate_metrics(y_test_adjusted, rf_pred, 'rf_')\n",
        "                    self.evaluation_results['rf'] = rf_metrics\n",
        "                    self.detailed_metrics['rf'] = {\n",
        "                        'predictions': rf_pred,\n",
        "                        'actual': y_test_adjusted,\n",
        "                        'metrics': rf_metrics\n",
        "                    }\n",
        "                    print(\"RF evaluation complete\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in RF evaluation: {str(e)}\")\n",
        "\n",
        "            # Evaluate XGBoost\n",
        "            if self.model_trainer.models.get('xgb') is not None:\n",
        "                try:\n",
        "                    print(\"Evaluating XGBoost...\")\n",
        "                    xgb_pred = self.model_trainer.models['xgb'].predict(X_test_adjusted)\n",
        "                    xgb_metrics = self._calculate_metrics(y_test_adjusted, xgb_pred, 'xgb_')\n",
        "                    self.evaluation_results['xgb'] = xgb_metrics\n",
        "                    self.detailed_metrics['xgb'] = {\n",
        "                        'predictions': xgb_pred,\n",
        "                        'actual': y_test_adjusted,\n",
        "                        'metrics': xgb_metrics\n",
        "                    }\n",
        "                    print(\"XGB evaluation complete\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in XGBoost evaluation: {str(e)}\")\n",
        "\n",
        "            # Create and evaluate ensemble\n",
        "            if len(self.detailed_metrics) >= 2 and self.model_trainer.ensemble_weights:\n",
        "                try:\n",
        "                    print(\"Creating ensemble predictions...\")\n",
        "                    ensemble_pred = np.zeros_like(y_test_adjusted, dtype=float)\n",
        "\n",
        "                    for model_name, weight in self.model_trainer.ensemble_weights.items():\n",
        "                        if model_name in self.detailed_metrics:\n",
        "                            ensemble_pred += weight * self.detailed_metrics[model_name]['predictions']\n",
        "\n",
        "                    ensemble_metrics = self._calculate_metrics(y_test_adjusted, ensemble_pred, 'ensemble_')\n",
        "                    self.evaluation_results['ensemble'] = ensemble_metrics\n",
        "                    self.detailed_metrics['ensemble'] = {\n",
        "                        'predictions': ensemble_pred,\n",
        "                        'actual': y_test_adjusted,\n",
        "                        'metrics': ensemble_metrics\n",
        "                    }\n",
        "                    print(\"Ensemble evaluation complete\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in ensemble evaluation: {str(e)}\")\n",
        "\n",
        "            return self.evaluation_results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in model evaluation: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return {'error': str(e)}\n",
        "\n",
        "    def _evaluate_tree_models(self, model_name, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Enhanced evaluation for tree-based models with detailed error analysis\n",
        "        \"\"\"\n",
        "        try:\n",
        "            model = self.model_trainer.models[model_name]\n",
        "            if model is None:\n",
        "                print(f\"Skipping {model_name} evaluation - model not trained\")\n",
        "                return\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred = model.predict(X_test)\n",
        "            if len(y_pred) == 0:\n",
        "                raise ValueError(f\"Empty predictions for {model_name}\")\n",
        "\n",
        "            # Calculate comprehensive metrics\n",
        "            metrics = self._calculate_comprehensive_metrics(y_test, y_pred, model_name)\n",
        "\n",
        "            # Calculate time-based metrics\n",
        "            time_metrics = self._calculate_time_based_metrics(y_test, y_pred)\n",
        "            metrics.update({f\"{model_name}_{k}\": v for k, v in time_metrics.items()})\n",
        "\n",
        "            # Feature importance analysis\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                feature_imp = pd.DataFrame({\n",
        "                    'feature': self.model_trainer.feature_cols,\n",
        "                    'importance': model.feature_importances_\n",
        "                }).sort_values('importance', ascending=False)\n",
        "\n",
        "                metrics[f'{model_name}_top_features'] = feature_imp.head(10).to_dict()\n",
        "                self.feature_importance_analysis[model_name] = feature_imp\n",
        "\n",
        "            # Error analysis\n",
        "            error_metrics = self._calculate_error_distribution(y_test, y_pred)\n",
        "            self.error_analysis_results[model_name] = error_metrics\n",
        "            metrics.update({f\"{model_name}_{k}\": v for k, v in error_metrics.items()})\n",
        "\n",
        "            # Store all results\n",
        "            self.evaluation_results[model_name] = metrics\n",
        "            self.detailed_metrics[model_name] = {\n",
        "                'predictions': y_pred,\n",
        "                'actual': y_test,\n",
        "                'metrics': metrics\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating {model_name}: {str(e)}\")\n",
        "            self.evaluation_results[model_name] = {\n",
        "                'error': f'Evaluation failed for {model_name}',\n",
        "                'error_message': str(e)\n",
        "            }\n",
        "\n",
        "    def _calculate_comprehensive_metrics(self, y_true, y_pred, prefix=''):\n",
        "        \"\"\"\n",
        "        Calculate comprehensive evaluation metrics\n",
        "        \"\"\"\n",
        "        metrics = {\n",
        "            f'{prefix}RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
        "            f'{prefix}MAE': mean_absolute_error(y_true, y_pred),\n",
        "            f'{prefix}R2': r2_score(y_true, y_pred),\n",
        "            f'{prefix}MAPE': np.mean(np.abs((y_true - y_pred) / y_true)) * 100,\n",
        "            f'{prefix}Normalized_RMSE': np.sqrt(mean_squared_error(y_true, y_pred)) / np.mean(y_true),\n",
        "            f'{prefix}Bias': np.mean(y_pred - y_true),\n",
        "            f'{prefix}Max_Error': np.max(np.abs(y_pred - y_true))\n",
        "        }\n",
        "\n",
        "        # Calculate additional metrics\n",
        "        metrics[f'{prefix}Median_AE'] = np.median(np.abs(y_true - y_pred))\n",
        "        metrics[f'{prefix}R2_Adjusted'] = 1 - (1 - metrics[f'{prefix}R2']) * (len(y_true) - 1) / (len(y_true) - len(self.model_trainer.feature_cols) - 1)\n",
        "        metrics[f'{prefix}RMSE_CV'] = np.sqrt(mean_squared_error(y_true, y_pred)) / np.mean(y_true) * 100\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _calculate_time_based_metrics(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate metrics for different time periods\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Daily aggregation\n",
        "        daily_true = pd.Series(y_true).rolling(window=96).mean()  # 24 hours * 4 (15-min intervals)\n",
        "        daily_pred = pd.Series(y_pred).rolling(window=96).mean()\n",
        "\n",
        "        # Weekly aggregation\n",
        "        weekly_true = pd.Series(y_true).rolling(window=96*7).mean()\n",
        "        weekly_pred = pd.Series(y_pred).rolling(window=96*7).mean()\n",
        "\n",
        "        # Calculate metrics for different time scales\n",
        "        metrics['Daily_RMSE'] = np.sqrt(mean_squared_error(daily_true.dropna(), daily_pred.dropna()))\n",
        "        metrics['Weekly_RMSE'] = np.sqrt(mean_squared_error(weekly_true.dropna(), weekly_pred.dropna()))\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def _calculate_error_distribution(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Analyze error distribution and patterns\n",
        "        \"\"\"\n",
        "        errors = y_true - y_pred\n",
        "        abs_errors = np.abs(errors)\n",
        "\n",
        "        metrics = {\n",
        "            'Error_Mean': np.mean(errors),\n",
        "            'Error_Std': np.std(errors),\n",
        "            'Error_Skew': pd.Series(errors).skew(),\n",
        "            'Error_Kurtosis': pd.Series(errors).kurtosis(),\n",
        "            'Error_Q1': np.percentile(abs_errors, 25),\n",
        "            'Error_Q3': np.percentile(abs_errors, 75),\n",
        "            'Error_IQR': np.percentile(abs_errors, 75) - np.percentile(abs_errors, 25)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def evaluate_lstm(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Enhanced LSTM evaluation with proper scaling handling\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if isinstance(y_test, pd.Series):\n",
        "                y_test = y_test.to_numpy()\n",
        "\n",
        "            # Handle NaN values\n",
        "            X_test = np.nan_to_num(X_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            y_test = np.nan_to_num(y_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "\n",
        "            # Scale test data\n",
        "            X_test_scaled = self.scalers['feature_scaler'].transform(X_test)\n",
        "            seq_length = self.scalers['seq_length']\n",
        "\n",
        "            # Create sequences\n",
        "            X_test_seq = []\n",
        "            y_test_seq = []\n",
        "\n",
        "            for i in range(len(X_test_scaled) - seq_length):\n",
        "                X_test_seq.append(X_test_scaled[i:i + seq_length])\n",
        "                y_test_seq.append(y_test[i + seq_length])\n",
        "\n",
        "            X_test_seq = np.array(X_test_seq)\n",
        "            y_test_seq = np.array(y_test_seq)\n",
        "\n",
        "            print(f\"Test sequence shapes - X: {X_test_seq.shape}, y: {y_test_seq.shape}\")\n",
        "\n",
        "            # Make predictions\n",
        "            y_pred_scaled = self.model_trainer.models['lstm'].predict(X_test_seq)\n",
        "\n",
        "            # Inverse transform predictions\n",
        "            y_pred = self.scalers['target_scaler'].inverse_transform(\n",
        "                y_pred_scaled).flatten()\n",
        "\n",
        "            print(f\"Final shapes - predictions: {y_pred.shape}, actual: {y_test_seq.shape}\")\n",
        "            print(f\"Range - predictions: [{y_pred.min():.2f}, {y_pred.max():.2f}], \"\n",
        "                  f\"actual: [{y_test_seq.min():.2f}, {y_test_seq.max():.2f}]\")\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = self._calculate_metrics(y_test_seq, y_pred, 'lstm_')\n",
        "            print(\"LSTM metrics calculated successfully\")\n",
        "\n",
        "            return metrics, y_pred, y_test_seq\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in LSTM evaluation: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return None, None, None\n",
        "\n",
        "    def _evaluate_ensemble(self, X_test, y_test):\n",
        "        \"\"\"\n",
        "        Evaluate ensemble model with weighted predictions\n",
        "        \"\"\"\n",
        "        if self.model_trainer.ensemble_weights is None:\n",
        "            print(\"Skipping ensemble evaluation - ensemble not created\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            # Get predictions from all models\n",
        "            predictions = {}\n",
        "\n",
        "            # Tree-based models\n",
        "            for model_name in ['rf', 'xgb']:\n",
        "                if self.model_trainer.models[model_name] is not None:\n",
        "                    predictions[model_name] = self.model_trainer.models[model_name].predict(X_test)\n",
        "\n",
        "            # LSTM\n",
        "            if self.model_trainer.models['lstm'] is not None:\n",
        "                X_test_seq, y_test_seq = self.model_trainer.prepare_sequence_data(X_test, y_test)\n",
        "                lstm_pred = self.model_trainer.models['lstm'].predict(X_test_seq).flatten()\n",
        "                predictions['lstm'] = lstm_pred[-len(predictions['rf']):]\n",
        "\n",
        "            # Calculate weighted ensemble predictions\n",
        "            ensemble_pred = np.zeros_like(predictions['rf'])\n",
        "            for model_name, weight in self.model_trainer.ensemble_weights.items():\n",
        "                if model_name in predictions:\n",
        "                    ensemble_pred += weight * predictions[model_name]\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = self.calculate_metrics(y_test[-len(ensemble_pred):],\n",
        "                                          ensemble_pred, 'ensemble_')\n",
        "\n",
        "            # Error analysis\n",
        "            self.error_analysis['ensemble'] = {\n",
        "                'residuals': y_test[-len(ensemble_pred):] - ensemble_pred,\n",
        "                'percent_error': ((y_test[-len(ensemble_pred):] - ensemble_pred) /\n",
        "                                y_test[-len(ensemble_pred):]) * 100,\n",
        "                'abs_error': np.abs(y_test[-len(ensemble_pred):] - ensemble_pred)\n",
        "            }\n",
        "\n",
        "            self.evaluation_results['ensemble'] = metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in ensemble evaluation: {str(e)}\")\n",
        "\n",
        "    def _perform_comparative_analysis(self):\n",
        "        \"\"\"\n",
        "        Perform comparative analysis between models\n",
        "        \"\"\"\n",
        "        # Compare models on common metrics\n",
        "        common_metrics = ['RMSE', 'MAE', 'R2', 'MAPE']\n",
        "        model_comparison = {}\n",
        "\n",
        "        for metric in common_metrics:\n",
        "            metric_values = {}\n",
        "            for model_name, metrics in self.evaluation_results.items():\n",
        "                metric_key = f'{model_name}_{metric}'\n",
        "                if metric_key in metrics:\n",
        "                    metric_values[model_name] = metrics[metric_key]\n",
        "            model_comparison[metric] = metric_values\n",
        "\n",
        "        self.evaluation_results['model_comparison'] = model_comparison\n",
        "\n",
        "    def generate_evaluation_plots(self):\n",
        "        \"\"\"\n",
        "        Fixed plot generation with proper variable handling\n",
        "        \"\"\"\n",
        "        plots = {}\n",
        "\n",
        "        try:\n",
        "            # Only create plots if we have valid metrics\n",
        "            if not self.detailed_metrics:\n",
        "                print(\"No metrics available for plotting\")\n",
        "                return {}\n",
        "\n",
        "            # 1. Prediction vs Actual plots\n",
        "            fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "            fig.suptitle('Prediction vs Actual Comparison', fontsize=16)\n",
        "\n",
        "            for idx, (model_name, metrics) in enumerate(self.detailed_metrics.items()):\n",
        "                if 'actual' not in metrics or 'predictions' not in metrics:\n",
        "                    continue\n",
        "\n",
        "                row = idx // 2\n",
        "                col = idx % 2\n",
        "\n",
        "                actual = metrics['actual']\n",
        "                pred = metrics['predictions']\n",
        "\n",
        "                # Create scatter plot\n",
        "                axes[row, col].scatter(actual, pred, alpha=0.5, label='Predictions')\n",
        "\n",
        "                # Add perfect prediction line\n",
        "                min_val = min(np.min(actual), np.min(pred))\n",
        "                max_val = max(np.max(actual), np.max(pred))\n",
        "                axes[row, col].plot([min_val, max_val], [min_val, max_val],\n",
        "                                  'r--', label='Perfect Prediction')\n",
        "\n",
        "                axes[row, col].set_title(f'{model_name.upper()} Predictions')\n",
        "                axes[row, col].set_xlabel('Actual Values')\n",
        "                axes[row, col].set_ylabel('Predicted Values')\n",
        "                axes[row, col].legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plots['prediction_comparison'] = plt.gcf()\n",
        "            plt.close()\n",
        "\n",
        "            # 2. Time series plot\n",
        "            fig, ax = plt.subplots(figsize=(20, 10))\n",
        "\n",
        "            # Get first model's actual values for reference\n",
        "            first_model = next(iter(self.detailed_metrics.values()))\n",
        "            if 'actual' in first_model:\n",
        "                reference_actual = first_model['actual'][:100]  # Plot first 100 points\n",
        "                ax.plot(reference_actual, 'k--', label='Actual', alpha=0.5)\n",
        "\n",
        "            # Plot predictions for each model\n",
        "            for model_name, metrics in self.detailed_metrics.items():\n",
        "                if 'predictions' in metrics:\n",
        "                    pred = metrics['predictions'][:100]\n",
        "                    ax.plot(pred, label=f'{model_name.upper()} Predictions')\n",
        "\n",
        "            ax.set_title('Time Series Comparison (First 100 points)')\n",
        "            ax.set_xlabel('Time Step')\n",
        "            ax.set_ylabel('Power Output')\n",
        "            ax.legend()\n",
        "            plt.tight_layout()\n",
        "            plots['time_series'] = plt.gcf()\n",
        "            plt.close()\n",
        "\n",
        "            return plots\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating evaluation plots: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return {}\n",
        "\n",
        "\n",
        "    def generate_evaluation_report(self):\n",
        "        \"\"\"\n",
        "        Generate a comprehensive evaluation report\n",
        "        \"\"\"\n",
        "        report = {\n",
        "            'summary': {\n",
        "                'best_model': None,\n",
        "                'best_score': float('-inf'),\n",
        "                'model_rankings': {}\n",
        "            },\n",
        "            'detailed_metrics': self.evaluation_results,\n",
        "            'error_analysis': {\n",
        "                model: {\n",
        "                    'mean_error': np.mean(data['residuals']),\n",
        "                    'std_error': np.std(data['residuals']),\n",
        "                    'max_error': np.max(np.abs(data['residuals'])),\n",
        "                    'error_95th_percentile': np.percentile(np.abs(data['residuals']), 95)\n",
        "                }\n",
        "                for model, data in self.error_analysis.items()\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Determine best model\n",
        "        for model_name, metrics in self.evaluation_results.items():\n",
        "            if model_name != 'model_comparison':\n",
        "                score = metrics.get(f'{model_name}_R2', float('-inf'))\n",
        "                report['summary']['model_rankings'][model_name] = score\n",
        "                if score > report['summary']['best_score']:\n",
        "                    report['summary']['best_score'] = score\n",
        "                    report['summary']['best_model'] = model_name\n",
        "\n",
        "        return report\n",
        "\n",
        "def main():\n",
        "    # Initialize data processor\n",
        "    data_processor = SolarPowerAnalysis()\n",
        "\n",
        "    # Load and process data\n",
        "    data_processor.load_data('/content/drive/MyDrive/Data mining/Plant_1_Generation_Data.csv', '/content/drive/MyDrive/Data mining/Plant_1_Weather_Sensor_Data.csv')\n",
        "    data_processor.engineer_features()\n",
        "\n",
        "    # Generate EDA plots\n",
        "    try:\n",
        "        eda_plots = data_processor.generate_eda_plots()\n",
        "        print(\"Generated EDA plots successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating EDA plots: {e}\")\n",
        "        eda_plots = {}\n",
        "\n",
        "    # Initialize model trainer\n",
        "    model_trainer = SolarPowerModeling(data_processor)\n",
        "\n",
        "    # Prepare data for modeling\n",
        "    model_trainer.prepare_data()\n",
        "\n",
        "    # Train models\n",
        "    model_trainer.train_random_forest()\n",
        "    model_trainer.train_xgboost()\n",
        "    model_trainer.train_lstm()\n",
        "    model_trainer.create_ensemble()\n",
        "\n",
        "    # Initialize evaluator\n",
        "    evaluator = ModelEvaluation(model_trainer)\n",
        "\n",
        "    # Evaluate models\n",
        "    evaluation_results = evaluator.evaluate_models()\n",
        "\n",
        "    # Generate evaluation plots\n",
        "    try:\n",
        "        evaluation_plots = evaluator.generate_evaluation_plots()\n",
        "        print(\"Generated evaluation plots successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating evaluation plots: {e}\")\n",
        "        evaluation_plots = {}\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nModel Evaluation Results:\")\n",
        "    for model_name, metrics in evaluation_results.items():\n",
        "        if model_name != 'model_comparison' and isinstance(metrics, dict):\n",
        "            print(f\"\\n{model_name.upper()} Results:\")\n",
        "            for metric_name, value in metrics.items():\n",
        "                try:\n",
        "                    if isinstance(value, dict):\n",
        "                        print(f\"{metric_name}:\")\n",
        "                        for k, v in value.items():\n",
        "                            if isinstance(v, (int, float)):\n",
        "                                print(f\"  {k}: {v:.4f}\")\n",
        "                            else:\n",
        "                                print(f\"  {k}: {v}\")\n",
        "                    elif isinstance(value, (int, float)):\n",
        "                        print(f\"{metric_name}: {value:.4f}\")\n",
        "                    else:\n",
        "                        print(f\"{metric_name}: {value}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error printing metric {metric_name}: {str(e)}\")\n",
        "\n",
        "    # Save models with error handling\n",
        "    for model_name, model in model_trainer.models.items():\n",
        "        if model is not None:\n",
        "            try:\n",
        "                if model_name != 'lstm':\n",
        "                    joblib.dump(model, f'{model_name}_model.joblib')\n",
        "                else:\n",
        "                    model.save(f'{model_name}_model.h5')\n",
        "                print(f\"Successfully saved {model_name} model\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving {model_name} model: {str(e)}\")\n",
        "\n",
        "    # Save plots\n",
        "    all_plots = {**eda_plots, **evaluation_plots}\n",
        "    for plot_name, fig in all_plots.items():\n",
        "        try:\n",
        "            fig.savefig(f'{plot_name}.png')\n",
        "            print(f\"Successfully saved plot: {plot_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving plot {plot_name}: {str(e)}\")\n",
        "\n",
        "    # Save evaluation results\n",
        "    try:\n",
        "        pd.DataFrame(evaluation_results).to_csv('model_evaluation_results.csv')\n",
        "        print(\"Successfully saved evaluation results\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving evaluation results: {str(e)}\")\n",
        "\n",
        "    # Print final results\n",
        "    print(\"\\nExecution complete. Results and plots have been saved.\")\n",
        "\n",
        "    # Determine best model based on RMSE\n",
        "    best_model = None\n",
        "    best_score = float('inf')\n",
        "    model_rankings = {}\n",
        "\n",
        "    for model_name, metrics in evaluation_results.items():\n",
        "        if model_name != 'model_comparison' and isinstance(metrics, dict):\n",
        "            rmse = metrics.get(f'{model_name}_RMSE')\n",
        "            if rmse is not None:\n",
        "                model_rankings[model_name] = rmse\n",
        "                if rmse < best_score:\n",
        "                    best_score = rmse\n",
        "                    best_model = model_name\n",
        "\n",
        "    print(\"\\nBest Model:\", best_model)\n",
        "    print(\"\\nModel Rankings:\")\n",
        "    for model_name, score in sorted(model_rankings.items(), key=lambda x: x[1]):\n",
        "        print(f\"{model_name}: RMSE = {score:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8as1jsDrg4N"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}